"""
Crear √≠ndice BM25 desde el √≠ndice FAISS existente
para b√∫squeda l√©xica complementaria
"""
import os
import pickle
import json
from pathlib import Path
from rank_bm25 import BM25Okapi
from tqdm import tqdm

os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'credencial json/midyear-node-436821-t3-525a146e96a0.json'

from langchain_google_vertexai import VertexAIEmbeddings
from langchain_community.vectorstores import FAISS

print("üîç Creando √≠ndice BM25 desde FAISS...\n")

# 1. Cargar FAISS
print("üì• Cargando √≠ndice FAISS...")
embeddings = VertexAIEmbeddings(
    model_name="text-multilingual-embedding-002",
    project="midyear-node-436821-t3"
)

faiss_vs = FAISS.load_local(
    folder_path="faiss_index",
    embeddings=embeddings,
    allow_dangerous_deserialization=True
)

print(f"‚úÖ FAISS cargado: {faiss_vs.index.ntotal:,} documentos\n")

# 2. Extraer textos y metadata
print("üìÑ Extrayendo textos y metadata...")
docs = []
metadatas = []

# Obtener todos los documentos del docstore
docstore = faiss_vs.docstore._dict
total = len(docstore)

for doc_id, doc in tqdm(docstore.items(), total=total, desc="Procesando"):
    docs.append(doc.page_content)
    metadatas.append(doc.metadata)

print(f"‚úÖ Extra√≠dos {len(docs):,} documentos\n")

# 3. Tokenizar para BM25 con limpieza de puntuaci√≥n
print("‚úÇÔ∏è  Tokenizando documentos...")
import re
tokenized_docs = []

def tokenize_clean(text):
    """Tokenizaci√≥n mejorada: lowercase + limpieza de puntuaci√≥n + split"""
    # Convertir a min√∫sculas
    text = text.lower()
    # Remover puntuaci√≥n pero mantener tildes y √±
    text = re.sub(r'[^\w\s√°√©√≠√≥√∫√±√º]', ' ', text)
    # Split y filtrar tokens vac√≠os
    tokens = [t for t in text.split() if t]
    return tokens

for text in tqdm(docs, desc="Tokenizando"):
    tokens = tokenize_clean(text)
    tokenized_docs.append(tokens)

print(f"‚úÖ Tokenizaci√≥n completada\n")

# 4. Crear √≠ndice BM25
print("üî® Construyendo √≠ndice BM25...")
bm25 = BM25Okapi(tokenized_docs)
print("‚úÖ √çndice BM25 creado\n")

# 5. Guardar √≠ndice BM25 y metadata
print("üíæ Guardando √≠ndice BM25...")

bm25_data = {
    'bm25': bm25,
    'docs': docs,
    'metadatas': metadatas
}

with open('bm25_index.pkl', 'wb') as f:
    pickle.dump(bm25_data, f)

print(f"‚úÖ √çndice guardado en bm25_index.pkl")

# Guardar estad√≠sticas
stats = {
    'total_docs': len(docs),
    'avg_doc_length': sum(len(td) for td in tokenized_docs) / len(tokenized_docs),
    'total_tokens': sum(len(td) for td in tokenized_docs)
}

with open('bm25_stats.json', 'w', encoding='utf-8') as f:
    json.dump(stats, f, indent=2)

print(f"‚úÖ Estad√≠sticas guardadas en bm25_stats.json\n")

# 6. Mostrar resumen
print("=" * 60)
print("üìä RESUMEN DEL √çNDICE BM25")
print("=" * 60)
print(f"Total documentos: {stats['total_docs']:,}")
print(f"Longitud promedio: {stats['avg_doc_length']:.1f} tokens")
print(f"Total tokens: {stats['total_tokens']:,}")
print(f"Tama√±o archivo: {Path('bm25_index.pkl').stat().st_size / (1024*1024):.2f} MB")
print("=" * 60)
print("\n‚ú® √çndice BM25 listo para b√∫squeda h√≠brida")
